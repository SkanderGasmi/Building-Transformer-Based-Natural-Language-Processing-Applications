{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment: Authorship Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorship attribution is a type of text classification problem.  Instead of categorizing text by _topic_, as you did in the disease text classification problem, the objective is to classify the text by _author_.  \n",
    "\n",
    "The inherent assumption in trying to solve a problem like this is that there is *some difference between the styles* of the authors in question, *which can be discerned by a model*.  Is that the case for BERT et al?  Is a language model able to \"understand\" written style? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "[The Problem](#The-Problem)<br>\n",
    "[Scoring](#Scoring)<br>\n",
    "[Step 1: Prepare the Data](#Step-1:-Prepare-the-Data)<br>\n",
    "[Step 2: Prepare the Model Configuration](#Step-2:-Prepare-the-Model-Configuration)<br>\n",
    "[Step 3: Prepare the Trainer Configuration](#Step-3:-Prepare-the-Trainer-Configuration)<br>\n",
    "[Step 4: Train](#Step-4:-Train)<br>\n",
    "[Step 5: Infer](#Step-5:-Infer)<br>\n",
    "[Step 6: Submit You Assessment](#Step-6:-Submit-You-Assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The Problem\n",
    "### The Federalist Papers - History Mystery!\n",
    "\n",
    "The [Federalist Papers](https://en.wikipedia.org/wiki/The_Federalist_Papers) are a set of essays written between 1787 and 1788 by [Alexander Hamilton](https://en.wikipedia.org/wiki/Alexander_Hamilton), [James Madison](https://en.wikipedia.org/wiki/James_Madison) and [John Jay](https://en.wikipedia.org/wiki/John_Jay).  Initially published under the pseudonym 'Publius', their intent was to encourage the ratification of the then-new Constitution of the United States of America.  In later years, a list emerged where the author of each one of the 85 papers was identified.  Nevertheless, for a subset of these papers the author is still in question.  The problem of the Federalist Papers authorship attribution has been a subject of much research in statistical NLP in the past.   Now you will try to solve this question with your own BERT-based project model.\n",
    "<img style=\"float: right;\" src=\"images/HandM.png\" width=400>\n",
    "                                                                                                           \n",
    "In concrete terms, the problem is identifying, for each one of the disputed papers, whether Alexander Hamilton or James Madison are the authors.  For this exercise, you can assume that each paper has a single author, i.e., that no collaboration took place (though *that* is not 100% certain!), and that each author has a well-defined writing style that is displayed across all the identified papers. \n",
    "\n",
    "### Your Project\n",
    "You are provided with labeled `train.tsv` and `dev.tsv` datasets for the project.  There are 10 test sets, one for each of the disputed papers.  All datasets are contained in the `data/federalist_papers_HM` directory.  \n",
    "\n",
    "Each \"sentence\" is actually a group of sentences of approximately 256 words.  The labels are '0' for HAMILTON and '1' for MADISON.  There are more papers by Hamilton in the example files than by Madison.  The validation set has been created with approximately the same distribution of the two labels as in the training set.\n",
    "\n",
    "Your task is to build neural networks using NeMo, as you did in Lab 2.  You'll train your model and test it.  Then you'll use provided collation code to see what answers your model gives to the \"history mystery\"! \n",
    "\n",
    "Along the way, you'll save code snippets that will be tested with the autograder once you are done.  Submission instructions are provided at the end of the notebook for this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Scoring\n",
    "You will be assessed on your ability to set up and train a model for the project, rather than the final result.  This coding assessment is worth 70 points, divided as follows:\n",
    "\n",
    "### Rubric\n",
    "\n",
    "| Step                                 | Graded                                                    | FIXMEs?  | Points |\n",
    "|--------------------------------------|-----------------------------------------------------------|----------|--------|\n",
    "| 1. Prepare the Project               | Fix data format (correct format)                          |  2       | 10     |\n",
    "| 2. Prepare the Model Configuration   | Set model parameters for override                         |  3       | 15     |\n",
    "| 3. Prepare the Trainer Configuration | Set trainer parameters for override                       |  3       | 15     |\n",
    "| 4. Train                             | Run the Trainer (training logs indicate training correct) |  4       | 20      |\n",
    "| 5. Infer                             | Run Inference (results indicate working project)          |  0       | 10     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although you are very capable at this point of building the project without any help at all, some scaffolding is provided, including specific names for variables.  This is for the benefit of the autograder, so please use these constructs for your assessment.  Also, this assessment tests the use of the command line method using the `text_classification_with_bert.py` script and configuration file overrides. You are free to change parameters such as model name, sequence length, batch size, learning rate, number of epochs, and so on to improve your model as you see fit.\n",
    "\n",
    "Once you are confident that you've built a reliable model, follow the instructions for submission at the end of the notebook.\n",
    "\n",
    "### Resources and Hints\n",
    "* **Example code:**<br>\n",
    "In the file browser at your left, you'll find the `lab2_reference_notebooks` directory.  This contains solution notebooks from Lab 2 for text classification and NER to use as examples.\n",
    "* **Language model (PRETRAINED_MODEL_NAME):**<br>\n",
    "You may find it useful to try different language models to better discern style.  Specifically, it may be that capitalization is important, which would mean you'd want to try a \"cased\" model.\n",
    "* **Maximum sequence length (MAX_SEQ_LEN):**<br>\n",
    "Values that can be used for MAX_SEQ_LENGTH are 64, 128, or 256.  Larger models (BERT-large, Megatron) may require a smaller MAX_SEQ_LENGTH to avoid an out-of-memory error.\n",
    "* **Number of Classes (NUM_CLASSES):**<br>\n",
    "For the Federalist Papers, we are only concerned with HAMILTON and MADISON.  The papers by John Jay have been excluded from the dataset.\n",
    "* **Batch size (BATCH_SIZE):**<br>\n",
    "Larger batch sizes train faster, but large language models tend to use up the available memory quickly.\n",
    "* **Memory usage:**<br>\n",
    "Some of the models are very large.   If you get \"RuntimeError: CUDA out of memory\" during training, you'll know you need to reduce the batch size, sequence length, and/or choose a smaller language model, restart the kernel, and try again from the beginning of the notebook.\n",
    "* **Accuracy and loss:**<br>\n",
    "It is definitely possible to achieve 95% or more model accuracy for this project.  In addition to changes in accuracy as the model trains, pay attention to the loss value.  You want the loss value to be dropping and getting very small for best results.\n",
    "* **Number of epochs (NUM_EPOCHS):**<br>\n",
    "You may need to run more epochs for your model (or not!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 1: Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful utilities for grading\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def get_latest_model():  \n",
    "    nemo_model_paths = glob.glob('nemo_experiments/TextClassification/*/checkpoints/*.nemo')\n",
    "    # Sort newest first\n",
    "    nemo_model_paths.sort(reverse=True)\n",
    "    return nemo_model_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is located in the data directory - see the list in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached_dev_nemo_format.tsv_BertTokenizer_256_30522_-1_0_False.pkl\n",
      "cached_train_nemo_format.tsv_BertTokenizer_256_30522_-1_0_True.pkl\n",
      "dev.tsv\n",
      "dev_nemo_format.tsv\n",
      "test.tsv\n",
      "test49.tsv\n",
      "test50.tsv\n",
      "test51.tsv\n",
      "test52.tsv\n",
      "test53.tsv\n",
      "test54.tsv\n",
      "test55.tsv\n",
      "test56.tsv\n",
      "test57.tsv\n",
      "test62.tsv\n",
      "train.tsv\n",
      "train_nemo_format.tsv\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/dli/task/data/federalist_papers_HM'\n",
    "!ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Format (graded)\n",
    "The data is not in the correct format for NeMo text classification.  Correct the data and save the new datasets in the DATA_DIR as `train_nemo_format.tsv` and `dev_nemo_format.tsv`.  You do not need to do anything with any of the test files.\n",
    "\n",
    "Complete the <i><strong style=\"color:green;\">#FIXME</strong></i> lines and run the save cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the format for train.tsv and dev.tsv\n",
    "#   and save the updates in train_nemo_format.tsv and dev_nemo_format.tsv\n",
    "\n",
    "#FIXME train.tsv format\n",
    "#FIXME dev.tsv format\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "train_file_path = os.path.join(DATA_DIR, 'train.tsv')\n",
    "dev_file_path = os.path.join(DATA_DIR, 'dev.tsv')\n",
    "\n",
    "# Read the original datasets\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "dev_data = pd.read_csv(dev_file_path, sep='\\t')\n",
    "\n",
    "# Rename the columns to match the NeMo format\n",
    "train_data.rename(columns={'sentence': 'text', 'label': 'label'}, inplace=True)\n",
    "dev_data.rename(columns={'sentence': 'text', 'label': 'label'}, inplace=True)\n",
    "\n",
    "# Save the updated datasets in NeMo format\n",
    "train_nemo_format_path = os.path.join(DATA_DIR, 'train_nemo_format.tsv')\n",
    "dev_nemo_format_path = os.path.join(DATA_DIR, 'dev_nemo_format.tsv')\n",
    "\n",
    "train_data.to_csv(train_nemo_format_path, sep='\\t', index=False)\n",
    "dev_data.to_csv(dev_nemo_format_path, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "train_nemo_format.tsv sample\n",
      "*****\n",
      "text\tlabel\n",
      "Concerning Dangers from Dissensions Between the States For the Independent Journal .To the People of the State of New York : THE three last numbers of this paper have been dedicated to an enumeration of the dangers to which we should be exposed , in a state of disunion , from the arms and arts of foreign nations .I shall now proceed to delineate dangers of a different and , perhaps , still more alarming kind -- those which will in all probability flow from dissensions between the States themselves , and from domestic factions and convulsions .These have been already in some instances slightly anticipated ; but they deserve a more particular and more full investigation .A man must be far gone in Utopian speculations who can seriously doubt that , if these States should either be wholly disunited , or only united in partial confederacies , the subdivisions into which they might be thrown would have frequent and violent contests with each other .To presume a want of motives for such contests as an argument against their existence , would be to forget that men are ambitious , vindictive , and rapacious .To look for a continuation of harmony between a number of independent , unconnected sovereignties in the same neighborhood , would be to disregard the uniform course of human events , and to set at defiance the accumulated experience of ages .The causes of hostility among nations are innumerable .\t0\n",
      "There are some which have a general and almost constant operation upon the collective bodies of society .Of this description are the love of power or the desire of pre-eminence and dominion -- the jealousy of power , or the desire of equality and safety .There are others which have a more circumscribed though an equally operative influence within their spheres .Such are the rivalships and competitions of commerce between commercial nations .And there are others , not less numerous than either of the former , which take their origin entirely in private passions ; in the attachments , enmities , interests , hopes , and fears of leading individuals in the communities of which they are members .Men of this class , whether the favorites of a king or of a people , have in too many instances abused the confidence they possessed ; and assuming the pretext of some public motive , have not scrupled to sacrifice the national tranquillity to personal advantage or personal gratification .The celebrated Pericles , in compliance with the resentment of a prostitute,1 at the expense of much of the blood and treasure of his countrymen , attacked , vanquished , and destroyed the city of the SAMNIANS .The same man , stimulated by private pique against the MEGARENSIANS,2 another nation of Greece , or to avoid a prosecution with which he was threatened as an accomplice of a supposed theft of the statuary Phidias,3 or to get rid of the accusations prepared to be brought against him for dissipating the funds of the state in the purchase of popularity,4 or from a combination of all these causes , was the primitive author of that famous and fatal war , distinguished in the Grecian annals by the name of the PELOPONNESIAN war ; which , after various vicissitudes , intermissions , and renewals , terminated in the ruin of the Athenian commonwealth .\t0\n",
      "\n",
      "\n",
      "*****\n",
      "dev_nemo_format.tsv sample\n",
      "*****\n",
      "text\tlabel\n",
      "There have been , if I may so express it , almost as many popular as royal wars .The cries of the nation and the importunities of their representatives have , upon various occasions , dragged their monarchs into war , or continued them in it , contrary to their inclinations , and sometimes contrary to the real interests of the State .In that memorable struggle for superiority between the rival houses of AUSTRIA and BOURBON , which so long kept Europe in a flame , it is well known that the antipathies of the English against the French , seconding the ambition , or rather the avarice , of a favorite leader,10 protracted the war beyond the limits marked out by sound policy , and for a considerable time in opposition to the views of the court .The wars of these two last-mentioned nations have in a great measure grown out of commercial considerations , -- the desire of supplanting and the fear of being supplanted , either in particular branches of traffic or in the general advantages of trade and navigation .From this summary of what has taken place in other countries , whose situations have borne the nearest resemblance to our own , what reason can we have to confide in those reveries which would seduce us into an expectation of peace and cordiality between the members of the present confederacy , in a state of separation ? Have we not already seen enough of the fallacy and extravagance of those idle theories which have amused us with promises of an exemption from the imperfections , weaknesses and evils incident to society in every shape ?\t0\n",
      "They would , at the same time , be necessitated to strengthen the executive arm of government , in doing which their constitutions would acquire a progressive direction toward monarchy .It is of the nature of war to increase the executive at the expense of the legislative authority .The expedients which have been mentioned would soon give the States or confederacies that made use of them a superiority over their neighbors .Small states , or states of less natural strength , under vigorous governments , and with the assistance of disciplined armies , have often triumphed over large states , or states of greater natural strength , which have been destitute of these advantages .Neither the pride nor the safety of the more important States or confederacies would permit them long to submit to this mortifying and adventitious superiority .They would quickly resort to means similar to those by which it had been effected , to reinstate themselves in their lost pre-eminence .Thus , we should , in a little time , see established in every part of this country the same engines of despotism which have been the scourge of the Old World .This , at least , would be the natural course of things ; and our reasonings will be the more likely to be just , in proportion as they are accommodated to this standard .These are not vague inferences drawn from supposed or speculative defects in a Constitution , the whole power of which is lodged in the hands of a people , or their representatives and delegates , but they are solid conclusions , drawn from the natural and necessary progress of human affairs .\t0\n"
     ]
    }
   ],
   "source": [
    "# check your work\n",
    "print(\"*****\\ntrain_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $DATA_DIR/train_nemo_format.tsv\n",
    "print(\"\\n\\n*****\\ndev_nemo_format.tsv sample\\n*****\")\n",
    "!head -n 3 $DATA_DIR/dev_nemo_format.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "import os.path\n",
    "DATA_DIR = '/dli/task/data/federalist_papers_HM'\n",
    "step1 = []\n",
    "try:\n",
    "    with open(os.path.join(DATA_DIR,'train_nemo_format.tsv')) as f:\n",
    "        content = f.readlines()\n",
    "        step1 += content[:2]\n",
    "    with open(os.path.join(DATA_DIR,'dev_nemo_format.tsv')) as f:\n",
    "        content = f.readlines()\n",
    "        step1 += content[:2]\n",
    "except:\n",
    "    pass\n",
    "                \n",
    "with open(\"my_assessment/step1.json\", \"w\") as outfile: \n",
    "    json.dump(step1, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: Prepare the Model Configuration\n",
    "Review the default model configuration and available language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nemo_path: text_classification_model.nemo\n",
      "tokenizer:\n",
      "  tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "  vocab_file: null\n",
      "  tokenizer_model: null\n",
      "  special_tokens: null\n",
      "language_model:\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  lm_checkpoint: null\n",
      "  config_file: null\n",
      "  config: null\n",
      "classifier_head:\n",
      "  num_output_layers: 2\n",
      "  fc_dropout: 0.1\n",
      "class_labels:\n",
      "  class_labels_file: null\n",
      "dataset:\n",
      "  num_classes: ???\n",
      "  do_lower_case: false\n",
      "  max_seq_length: 256\n",
      "  class_balancing: null\n",
      "  use_cache: false\n",
      "train_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: true\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "validation_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "test_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "optim:\n",
      "  name: adam\n",
      "  lr: 2.0e-05\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "  weight_decay: 0.01\n",
      "  sched:\n",
      "    name: WarmupAnnealing\n",
      "    warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    monitor: val_loss\n",
      "    reduce_on_plateau: false\n",
      "infer_samples:\n",
      "- by the end of no such thing the audience , like beatrice , has a watchful affection\n",
      "  for the monster .\n",
      "- director rob marshall went out gunning to make a great one .\n",
      "- uneasy mishmash of styles and genres .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the default model portion of the config file\n",
    "CONFIG_DIR = \"/dli/task/nemo/examples/nlp/text_classification/conf\"\n",
    "CONFIG_FILE = \"text_classification_config.yaml\"\n",
    "\n",
    "config = OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['megatron-bert-345m-uncased',\n",
       " 'megatron-bert-345m-cased',\n",
       " 'megatron-bert-uncased',\n",
       " 'megatron-bert-cased',\n",
       " 'biomegatron-bert-345m-uncased',\n",
       " 'biomegatron-bert-345m-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc',\n",
       " 'bert-base-german-dbmdz-cased',\n",
       " 'bert-base-german-dbmdz-uncased',\n",
       " 'cl-tohoku/bert-base-japanese',\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       " 'cl-tohoku/bert-base-japanese-char',\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
       " 'wietsedv/bert-base-dutch-cased',\n",
       " 'distilbert-base-uncased',\n",
       " 'distilbert-base-uncased-distilled-squad',\n",
       " 'distilbert-base-cased',\n",
       " 'distilbert-base-cased-distilled-squad',\n",
       " 'distilbert-base-german-cased',\n",
       " 'distilbert-base-multilingual-cased',\n",
       " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       " 'roberta-base',\n",
       " 'roberta-large',\n",
       " 'roberta-large-mnli',\n",
       " 'distilroberta-base',\n",
       " 'roberta-base-openai-detector',\n",
       " 'roberta-large-openai-detector',\n",
       " 'albert-base-v1',\n",
       " 'albert-large-v1',\n",
       " 'albert-xlarge-v1',\n",
       " 'albert-xxlarge-v1',\n",
       " 'albert-base-v2',\n",
       " 'albert-large-v2',\n",
       " 'albert-xlarge-v2',\n",
       " 'albert-xxlarge-v2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what BERT-like language models are available\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "nemo_nlp.modules.get_pretrained_lm_models_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters (graded)\n",
    "Complete the <i><strong style=\"color:green;\">#FIXME</strong></i> lines and run the save cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the values\n",
    "# NUM_CLASSES = #FIXME \n",
    "# MAX_SEQ_LENGTH = #FIXME \n",
    "# BATCH_SIZE = #FIXME \n",
    "\n",
    "NUM_CLASSES = 2  #  # Since you are classifying between HAMILTON and MADISON\n",
    "MAX_SEQ_LENGTH = 256  # You can choose the sequence length that fits your GPU memory\n",
    "BATCH_SIZE = 64 # You can adjust the batch size based on your GPU memory\n",
    "\n",
    "PATH_TO_TRAIN_FILE = \"/dli/task/data/federalist_papers_HM/train_nemo_format.tsv\"\n",
    "PATH_TO_VAL_FILE = \"/dli/task/data/federalist_papers_HM/dev_nemo_format.tsv\"\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased' # change as desired\n",
    "LR = 1e-4 # change as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "with open(\"my_assessment/step2.json\", \"w\") as outfile: \n",
    "    json.dump([MAX_SEQ_LENGTH, NUM_CLASSES, BATCH_SIZE], outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 3: Prepare the Trainer Configuration\n",
    "Review the default trainer and exp_manager configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 100\n",
      "max_steps: null\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "amp_level: O0\n",
      "precision: 32\n",
      "accelerator: ddp\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "resume_from_checkpoint: null\n",
      "num_sanity_val_steps: 0\n",
      "checkpoint_callback: false\n",
      "logger: false\n",
      "\n",
      "exp_dir: null\n",
      "name: TextClassification\n",
      "create_tensorboard_logger: true\n",
      "create_checkpoint_callback: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config.trainer))\n",
    "print(OmegaConf.to_yaml(config.exp_manager))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters (graded)\n",
    "Set the automatic mixed precision to level 1 with FP16 precision.  Set the MAX_EPOCHS to a reasonable level, perhaps between 5 and 20.  <br>Complete the <i><strong style=\"color:green;\">#FIXME</strong></i> lines and run the save cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the values\n",
    "MAX_EPOCHS = 15\n",
    "AMP_LEVEL = \"O1\" \n",
    "PRECISION = 16  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to save for assessment - DO NOT CHANGE\n",
    "with open(\"my_assessment/step3.json\", \"w\") as outfile: \n",
    "    json.dump([MAX_EPOCHS, AMP_LEVEL, PRECISION], outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Step 4: Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Trainer (graded)\n",
    "Complete the <i><strong style=\"color:green;\">#FIXME</strong></i> in the following cell for train and validation batch sizes, amp level, and precision.  Then train and run the save cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-10-26 09:45:48 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/omegaconf/basecontainer.py:225: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "    Use OmegaConf.to_yaml(cfg)\n",
      "    \n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo I 2023-10-26 09:45:48 text_classification_with_bert:110] \n",
      "    Config Params:\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 30\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 0.0\n",
      "      amp_level: O1\n",
      "      precision: 16\n",
      "      accelerator: ddp\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      resume_from_checkpoint: null\n",
      "      num_sanity_val_steps: 0\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "    model:\n",
      "      nemo_path: text_classification_model.nemo\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: null\n",
      "        tokenizer_model: null\n",
      "        special_tokens: null\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-uncased\n",
      "        lm_checkpoint: null\n",
      "        config_file: null\n",
      "        config: null\n",
      "      classifier_head:\n",
      "        num_output_layers: 2\n",
      "        fc_dropout: 0.1\n",
      "      class_labels:\n",
      "        class_labels_file: null\n",
      "      dataset:\n",
      "        num_classes: 2\n",
      "        do_lower_case: false\n",
      "        max_seq_length: 256\n",
      "        class_balancing: null\n",
      "        use_cache: false\n",
      "      train_ds:\n",
      "        file_path: /dli/task/data/federalist_papers_HM/train_nemo_format.tsv\n",
      "        batch_size: 64\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      validation_ds:\n",
      "        file_path: /dli/task/data/federalist_papers_HM/dev_nemo_format.tsv\n",
      "        batch_size: 64\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      test_ds:\n",
      "        file_path: null\n",
      "        batch_size: 64\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        num_workers: 3\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 2.0e-05\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: WarmupAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          last_epoch: -1\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      infer_samples: []\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: TextClassification\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "[NeMo I 2023-10-26 09:45:48 exp_manager:216] Experiments will be logged at /dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48\n",
      "[NeMo I 2023-10-26 09:45:48 exp_manager:563] TensorboardLogger has been set up\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:120] Read 503 examples from /dli/task/data/federalist_papers_HM/train_nemo_format.tsv.\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:239] example 0: ['A', 'single', 'assembly', 'may', 'be', 'a', 'proper', 'receptacle', 'of', 'those', 'slender', ',', 'or', 'rather', 'fettered', ',', 'authorities', ',', 'which', 'have', 'been', 'heretofore', 'delegated', 'to', 'the', 'federal', 'head', ';', 'but', 'it', 'would', 'be', 'inconsistent', 'with', 'all', 'the', 'principles', 'of', 'good', 'government', ',', 'to', 'intrust', 'it', 'with', 'those', 'additional', 'powers', 'which', ',', 'even', 'the', 'moderate', 'and', 'more', 'rational', 'adversaries', 'of', 'the', 'proposed', 'Constitution', 'admit', ',', 'ought', 'to', 'reside', 'in', 'the', 'United', 'States', '.If', 'that', 'plan', 'should', 'not', 'be', 'adopted', ',', 'and', 'if', 'the', 'necessity', 'of', 'the', 'Union', 'should', 'be', 'able', 'to', 'withstand', 'the', 'ambitious', 'aims', 'of', 'those', 'men', 'who', 'may', 'indulge', 'magnificent', 'schemes', 'of', 'personal', 'aggrandizement', 'from', 'its', 'dissolution', ',', 'the', 'probability', 'would', 'be', ',', 'that', 'we', 'should', 'run', 'into', 'the', 'project', 'of', 'conferring', 'supplementary', 'powers', 'upon', 'Congress', ',', 'as', 'they', 'are', 'now', 'constituted', ';', 'and', 'either', 'the', 'machine', ',', 'from', 'the', 'intrinsic', 'feebleness', 'of', 'its', 'structure', ',', 'will', 'moulder', 'into', 'pieces', ',', 'in', 'spite', 'of', 'our', 'ill-judged', 'efforts', 'to', 'prop', 'it', ';', 'or', ',', 'by', 'successive', 'augmentations', 'of', 'its', 'force', 'an', 'energy', ',', 'as', 'necessity', 'might', 'prompt', ',', 'we', 'shall', 'finally', 'accumulate', ',', 'in', 'a', 'single', 'body', ',', 'all', 'the', 'most', 'important', 'prerogatives', 'of', 'sovereignty', ',', 'and', 'thus', 'entail', 'upon', 'our', 'posterity', 'one', 'of', 'the', 'most', 'execrable', 'forms', 'of', 'government', 'that', 'human', 'infatuation', 'ever', 'contrived', '.Thus', ',', 'we', 'should', 'create', 'in', 'reality', 'that', 'very', 'tyranny', 'which', 'the', 'adversaries', 'of', 'the', 'new', 'Constitution', 'either', 'are', ',', 'or', 'affect', 'to', 'be', ',', 'solicitous', 'to', 'avert', '.It', 'has', 'not', 'a', 'little', 'contributed', 'to', 'the', 'infirmities', 'of', 'the', 'existing', 'federal', 'system', ',', 'that', 'it', 'never', 'had', 'a', 'ratification', 'by', 'the', 'PEOPLE', '.']\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:240] subtokens: [CLS] a single assembly may be a proper rec ##ept ##acle of those slender , or rather fe ##ttered , authorities , which have been here ##to ##for ##e delegate ##d to the federal head ; but it would be inconsistent with all the principles of good government , to int ##rus ##t it with those additional powers which , even the moderate and more rational ad ##vers ##aries of the proposed constitution admit , ought to reside in the united states . if that plan should not be adopted , and if the necessity of the union should be able to withstand the ambitious aims of those men who may ind ##ul ##ge magnificent schemes of personal ag ##gra ##ndi ##ze ##ment from its dissolution , the probability would be , that we should run into the project of con ##fer ##ring supplementary powers upon congress , as they are now constituted ; and either the machine , from the intrinsic fee ##ble ##ness of its structure , will mo ##uld ##er into pieces , in spite of our ill - judged efforts to prop it ; or , by successive aug ##ment ##ations of its force an energy , as necessity might prompt , we shall finally accumulate , in a single body , all the most important pre ##ro ##gative ##s of sovereignty , and thus en ##tail upon our poster ##ity one of the most ex ##ec ##rable forms of government that human in ##fat ##uation ever con ##tri ##ved . thus [SEP]\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:241] input_ids: 101 1037 2309 3320 2089 2022 1037 5372 28667 23606 18630 1997 2216 10944 1010 2030 2738 10768 14795 1010 4614 1010 2029 2031 2042 2182 3406 29278 2063 11849 2094 2000 1996 2976 2132 1025 2021 2009 2052 2022 20316 2007 2035 1996 6481 1997 2204 2231 1010 2000 20014 7946 2102 2009 2007 2216 3176 4204 2029 1010 2130 1996 8777 1998 2062 11581 4748 14028 12086 1997 1996 3818 4552 6449 1010 11276 2000 13960 1999 1996 2142 2163 1012 2065 2008 2933 2323 2025 2022 4233 1010 1998 2065 1996 13185 1997 1996 2586 2323 2022 2583 2000 19319 1996 12479 8704 1997 2216 2273 2040 2089 27427 5313 3351 12047 11683 1997 3167 12943 17643 16089 4371 3672 2013 2049 12275 1010 1996 9723 2052 2022 1010 2008 2057 2323 2448 2046 1996 2622 1997 9530 7512 4892 26215 4204 2588 3519 1010 2004 2027 2024 2085 11846 1025 1998 2593 1996 3698 1010 2013 1996 23807 7408 3468 2791 1997 2049 3252 1010 2097 9587 21285 2121 2046 4109 1010 1999 8741 1997 2256 5665 1011 13224 4073 2000 17678 2009 1025 2030 1010 2011 11165 15476 3672 10708 1997 2049 2486 2019 2943 1010 2004 13185 2453 25732 1010 2057 4618 2633 27598 1010 1999 1037 2309 2303 1010 2035 1996 2087 2590 3653 3217 26792 2015 1997 12601 1010 1998 2947 4372 14162 2588 2256 13082 3012 2028 1997 1996 2087 4654 8586 16670 3596 1997 2231 2008 2529 1999 27753 14505 2412 9530 18886 7178 1012 2947 102\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:239] example 1: ['The', 'Achaean', 'league', 'received', 'its', 'first', 'birth', 'from', 'Achaeus', ',', 'and', 'its', 'second', 'from', 'Aratus', '.What', 'degree', 'of', 'agency', 'these', 'reputed', 'lawgivers', 'might', 'have', 'in', 'their', 'respective', 'establishments', ',', 'or', 'how', 'far', 'they', 'might', 'be', 'clothed', 'with', 'the', 'legitimate', 'authority', 'of', 'the', 'people', ',', 'can', 'not', 'in', 'every', 'instance', 'be', 'ascertained', '.In', 'some', ',', 'however', ',', 'the', 'proceeding', 'was', 'strictly', 'regular', '.Draco', 'appears', 'to', 'have', 'been', 'intrusted', 'by', 'the', 'people', 'of', 'Athens', 'with', 'indefinite', 'powers', 'to', 'reform', 'its', 'government', 'and', 'laws', '.And', 'Solon', ',', 'according', 'to', 'Plutarch', ',', 'was', 'in', 'a', 'manner', 'compelled', ',', 'by', 'the', 'universal', 'suffrage', 'of', 'his', 'fellow-citizens', ',', 'to', 'take', 'upon', 'him', 'the', 'sole', 'and', 'absolute', 'power', 'of', 'new-modeling', 'the', 'constitution', '.The', 'proceedings', 'under', 'Lycurgus', 'were', 'less', 'regular', ';', 'but', 'as', 'far', 'as', 'the', 'advocates', 'for', 'a', 'regular', 'reform', 'could', 'prevail', ',', 'they', 'all', 'turned', 'their', 'eyes', 'towards', 'the', 'single', 'efforts', 'of', 'that', 'celebrated', 'patriot', 'and', 'sage', ',', 'instead', 'of', 'seeking', 'to', 'bring', 'about', 'a', 'revolution', 'by', 'the', 'intervention', 'of', 'a', 'deliberative', 'body', 'of', 'citizens', '.Whence', 'could', 'it', 'have', 'proceeded', ',', 'that', 'a', 'people', ',', 'jealous', 'as', 'the', 'Greeks', 'were', 'of', 'their', 'liberty', ',', 'should', 'so', 'far', 'abandon', 'the', 'rules', 'of', 'caution', 'as', 'to', 'place', 'their', 'destiny', 'in', 'the', 'hands', 'of', 'a', 'single', 'citizen', '?', 'Whence', 'could', 'it', 'have', 'proceeded', ',', 'that', 'the', 'Athenians', ',', 'a', 'people', 'who', 'would', 'not', 'suffer', 'an', 'army', 'to', 'be', 'commanded', 'by', 'fewer', 'than', 'ten', 'generals', ',', 'and', 'who', 'required', 'no', 'other', 'proof', 'of', 'danger', 'to', 'their', 'liberties', 'than', 'the', 'illustrious', 'merit', 'of', 'a', 'fellow-citizen', ',', 'should', 'consider', 'one', 'illustrious', 'citizen', 'as', 'a', 'more', 'eligible', 'depositary', 'of', 'the', 'fortunes', 'of', 'themselves', 'and', 'their', 'posterity', ',', 'than', 'a', 'select', 'body', 'of', 'citizens', ',', 'from', 'whose', 'common', 'deliberations', 'more', 'wisdom', ',', 'as', 'well', 'as', 'more', 'safety', ',', 'might', 'have', 'been', 'expected', '?']\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:240] subtokens: [CLS] the ac ##hae ##an league received its first birth from ac ##hae ##us , and its second from ara ##tus . what degree of agency these reputed law ##gi ##vers might have in their respective establishments , or how far they might be clothed with the legitimate authority of the people , can not in every instance be as ##cer ##tained . in some , however , the proceeding was strictly regular . dr ##aco appears to have been int ##rus ##ted by the people of athens with indefinite powers to reform its government and laws . and solo ##n , according to pl ##uta ##rch , was in a manner compelled , by the universal suffrage of his fellow - citizens , to take upon him the sole and absolute power of new - modeling the constitution . the proceedings under l ##y ##cu ##rg ##us were less regular ; but as far as the advocates for a regular reform could pre ##va ##il , they all turned their eyes towards the single efforts of that celebrated patriot and sage , instead of seeking to bring about a revolution by the intervention of a del ##ibe ##rative body of citizens . when ##ce could it have proceeded , that a people , jealous as the greeks were of their liberty , should so far abandon the rules of caution as to place their destiny in the hands of a single citizen ? when ##ce could it have proceeded , that the athenian ##s , [SEP]\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:241] input_ids: 101 1996 9353 25293 2319 2223 2363 2049 2034 4182 2013 9353 25293 2271 1010 1998 2049 2117 2013 19027 5809 1012 2054 3014 1997 4034 2122 22353 2375 5856 14028 2453 2031 1999 2037 7972 17228 1010 2030 2129 2521 2027 2453 2022 24963 2007 1996 11476 3691 1997 1996 2111 1010 2064 2025 1999 2296 6013 2022 2004 17119 28055 1012 1999 2070 1010 2174 1010 1996 18207 2001 9975 3180 1012 2852 22684 3544 2000 2031 2042 20014 7946 3064 2011 1996 2111 1997 7571 2007 25617 4204 2000 5290 2049 2231 1998 4277 1012 1998 3948 2078 1010 2429 2000 20228 13210 11140 1010 2001 1999 1037 5450 15055 1010 2011 1996 5415 15178 1997 2010 3507 1011 4480 1010 2000 2202 2588 2032 1996 7082 1998 7619 2373 1997 2047 1011 11643 1996 4552 1012 1996 8931 2104 1048 2100 10841 10623 2271 2020 2625 3180 1025 2021 2004 2521 2004 1996 13010 2005 1037 3180 5290 2071 3653 3567 4014 1010 2027 2035 2357 2037 2159 2875 1996 2309 4073 1997 2008 6334 16419 1998 10878 1010 2612 1997 6224 2000 3288 2055 1037 4329 2011 1996 8830 1997 1037 3972 20755 18514 2303 1997 4480 1012 2043 3401 2071 2009 2031 8979 1010 2008 1037 2111 1010 9981 2004 1996 13176 2020 1997 2037 7044 1010 2323 2061 2521 10824 1996 3513 1997 14046 2004 2000 2173 2037 10461 1999 1996 2398 1997 1037 2309 6926 1029 2043 3401 2071 2009 2031 8979 1010 2008 1996 26956 2015 1010 102\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-10-26 09:45:49 text_classification_dataset:244] label: 1\n",
      "[NeMo W 2023-10-26 09:45:57 text_classification_dataset:250] Found 502 out of 502 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2023-10-26 09:45:57 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-10-26 09:45:57 data_preprocessing:301] Min: 257 |                  Max: 257 |                  Mean: 257.0 |                  Median: 257.0\n",
      "[NeMo I 2023-10-26 09:45:57 data_preprocessing:307] 75 percentile: 257.00\n",
      "[NeMo I 2023-10-26 09:45:57 data_preprocessing:308] 99 percentile: 257.00\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:120] Read 116 examples from /dli/task/data/federalist_papers_HM/dev_nemo_format.tsv.\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:239] example 0: ['There', 'have', 'been', ',', 'if', 'I', 'may', 'so', 'express', 'it', ',', 'almost', 'as', 'many', 'popular', 'as', 'royal', 'wars', '.The', 'cries', 'of', 'the', 'nation', 'and', 'the', 'importunities', 'of', 'their', 'representatives', 'have', ',', 'upon', 'various', 'occasions', ',', 'dragged', 'their', 'monarchs', 'into', 'war', ',', 'or', 'continued', 'them', 'in', 'it', ',', 'contrary', 'to', 'their', 'inclinations', ',', 'and', 'sometimes', 'contrary', 'to', 'the', 'real', 'interests', 'of', 'the', 'State', '.In', 'that', 'memorable', 'struggle', 'for', 'superiority', 'between', 'the', 'rival', 'houses', 'of', 'AUSTRIA', 'and', 'BOURBON', ',', 'which', 'so', 'long', 'kept', 'Europe', 'in', 'a', 'flame', ',', 'it', 'is', 'well', 'known', 'that', 'the', 'antipathies', 'of', 'the', 'English', 'against', 'the', 'French', ',', 'seconding', 'the', 'ambition', ',', 'or', 'rather', 'the', 'avarice', ',', 'of', 'a', 'favorite', 'leader,10', 'protracted', 'the', 'war', 'beyond', 'the', 'limits', 'marked', 'out', 'by', 'sound', 'policy', ',', 'and', 'for', 'a', 'considerable', 'time', 'in', 'opposition', 'to', 'the', 'views', 'of', 'the', 'court', '.The', 'wars', 'of', 'these', 'two', 'last-mentioned', 'nations', 'have', 'in', 'a', 'great', 'measure', 'grown', 'out', 'of', 'commercial', 'considerations', ',', '--', 'the', 'desire', 'of', 'supplanting', 'and', 'the', 'fear', 'of', 'being', 'supplanted', ',', 'either', 'in', 'particular', 'branches', 'of', 'traffic', 'or', 'in', 'the', 'general', 'advantages', 'of', 'trade', 'and', 'navigation', '.From', 'this', 'summary', 'of', 'what', 'has', 'taken', 'place', 'in', 'other', 'countries', ',', 'whose', 'situations', 'have', 'borne', 'the', 'nearest', 'resemblance', 'to', 'our', 'own', ',', 'what', 'reason', 'can', 'we', 'have', 'to', 'confide', 'in', 'those', 'reveries', 'which', 'would', 'seduce', 'us', 'into', 'an', 'expectation', 'of', 'peace', 'and', 'cordiality', 'between', 'the', 'members', 'of', 'the', 'present', 'confederacy', ',', 'in', 'a', 'state', 'of', 'separation', '?', 'Have', 'we', 'not', 'already', 'seen', 'enough', 'of', 'the', 'fallacy', 'and', 'extravagance', 'of', 'those', 'idle', 'theories', 'which', 'have', 'amused', 'us', 'with', 'promises', 'of', 'an', 'exemption', 'from', 'the', 'imperfections', ',', 'weaknesses', 'and', 'evils', 'incident', 'to', 'society', 'in', 'every', 'shape', '?']\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:240] subtokens: [CLS] there have been , if i may so express it , almost as many popular as royal wars . the cries of the nation and the import ##uni ##ties of their representatives have , upon various occasions , dragged their monarchs into war , or continued them in it , contrary to their inclination ##s , and sometimes contrary to the real interests of the state . in that memorable struggle for superiority between the rival houses of austria and bourbon , which so long kept europe in a flame , it is well known that the anti ##path ##ies of the english against the french , second ##ing the ambition , or rather the ava ##rice , of a favorite leader , 10 pro ##tracted the war beyond the limits marked out by sound policy , and for a considerable time in opposition to the views of the court . the wars of these two last - mentioned nations have in a great measure grown out of commercial considerations , - - the desire of su ##pp ##lan ##ting and the fear of being su ##pp ##lan ##ted , either in particular branches of traffic or in the general advantages of trade and navigation . from this summary of what has taken place in other countries , whose situations have borne the nearest resemblance to our own , what reason can we have to con ##fide in those rev ##eries which would seduce us into an expectation of peace and cord ##ial ##ity between [SEP]\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:241] input_ids: 101 2045 2031 2042 1010 2065 1045 2089 2061 4671 2009 1010 2471 2004 2116 2759 2004 2548 5233 1012 1996 12842 1997 1996 3842 1998 1996 12324 19496 7368 1997 2037 4505 2031 1010 2588 2536 6642 1010 7944 2037 19799 2046 2162 1010 2030 2506 2068 1999 2009 1010 10043 2000 2037 21970 2015 1010 1998 2823 10043 2000 1996 2613 5426 1997 1996 2110 1012 1999 2008 13432 5998 2005 19113 2090 1996 6538 3506 1997 5118 1998 15477 1010 2029 2061 2146 2921 2885 1999 1037 8457 1010 2009 2003 2092 2124 2008 1996 3424 15069 3111 1997 1996 2394 2114 1996 2413 1010 2117 2075 1996 16290 1010 2030 2738 1996 10927 17599 1010 1997 1037 5440 3003 1010 2184 4013 24301 1996 2162 3458 1996 6537 4417 2041 2011 2614 3343 1010 1998 2005 1037 6196 2051 1999 4559 2000 1996 5328 1997 1996 2457 1012 1996 5233 1997 2122 2048 2197 1011 3855 3741 2031 1999 1037 2307 5468 4961 2041 1997 3293 16852 1010 1011 1011 1996 4792 1997 10514 9397 5802 3436 1998 1996 3571 1997 2108 10514 9397 5802 3064 1010 2593 1999 3327 5628 1997 4026 2030 1999 1996 2236 12637 1997 3119 1998 9163 1012 2013 2023 12654 1997 2054 2038 2579 2173 1999 2060 3032 1010 3005 8146 2031 15356 1996 7205 14062 2000 2256 2219 1010 2054 3114 2064 2057 2031 2000 9530 20740 1999 2216 7065 28077 2029 2052 23199 2149 2046 2019 17626 1997 3521 1998 11601 4818 3012 2090 102\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:239] example 1: ['They', 'would', ',', 'at', 'the', 'same', 'time', ',', 'be', 'necessitated', 'to', 'strengthen', 'the', 'executive', 'arm', 'of', 'government', ',', 'in', 'doing', 'which', 'their', 'constitutions', 'would', 'acquire', 'a', 'progressive', 'direction', 'toward', 'monarchy', '.It', 'is', 'of', 'the', 'nature', 'of', 'war', 'to', 'increase', 'the', 'executive', 'at', 'the', 'expense', 'of', 'the', 'legislative', 'authority', '.The', 'expedients', 'which', 'have', 'been', 'mentioned', 'would', 'soon', 'give', 'the', 'States', 'or', 'confederacies', 'that', 'made', 'use', 'of', 'them', 'a', 'superiority', 'over', 'their', 'neighbors', '.Small', 'states', ',', 'or', 'states', 'of', 'less', 'natural', 'strength', ',', 'under', 'vigorous', 'governments', ',', 'and', 'with', 'the', 'assistance', 'of', 'disciplined', 'armies', ',', 'have', 'often', 'triumphed', 'over', 'large', 'states', ',', 'or', 'states', 'of', 'greater', 'natural', 'strength', ',', 'which', 'have', 'been', 'destitute', 'of', 'these', 'advantages', '.Neither', 'the', 'pride', 'nor', 'the', 'safety', 'of', 'the', 'more', 'important', 'States', 'or', 'confederacies', 'would', 'permit', 'them', 'long', 'to', 'submit', 'to', 'this', 'mortifying', 'and', 'adventitious', 'superiority', '.They', 'would', 'quickly', 'resort', 'to', 'means', 'similar', 'to', 'those', 'by', 'which', 'it', 'had', 'been', 'effected', ',', 'to', 'reinstate', 'themselves', 'in', 'their', 'lost', 'pre-eminence', '.Thus', ',', 'we', 'should', ',', 'in', 'a', 'little', 'time', ',', 'see', 'established', 'in', 'every', 'part', 'of', 'this', 'country', 'the', 'same', 'engines', 'of', 'despotism', 'which', 'have', 'been', 'the', 'scourge', 'of', 'the', 'Old', 'World', '.This', ',', 'at', 'least', ',', 'would', 'be', 'the', 'natural', 'course', 'of', 'things', ';', 'and', 'our', 'reasonings', 'will', 'be', 'the', 'more', 'likely', 'to', 'be', 'just', ',', 'in', 'proportion', 'as', 'they', 'are', 'accommodated', 'to', 'this', 'standard', '.These', 'are', 'not', 'vague', 'inferences', 'drawn', 'from', 'supposed', 'or', 'speculative', 'defects', 'in', 'a', 'Constitution', ',', 'the', 'whole', 'power', 'of', 'which', 'is', 'lodged', 'in', 'the', 'hands', 'of', 'a', 'people', ',', 'or', 'their', 'representatives', 'and', 'delegates', ',', 'but', 'they', 'are', 'solid', 'conclusions', ',', 'drawn', 'from', 'the', 'natural', 'and', 'necessary', 'progress', 'of', 'human', 'affairs', '.']\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:240] subtokens: [CLS] they would , at the same time , be necessitated to strengthen the executive arm of government , in doing which their constitution ##s would acquire a progressive direction toward monarchy . it is of the nature of war to increase the executive at the expense of the legislative authority . the ex ##ped ##ient ##s which have been mentioned would soon give the states or con ##fed ##era ##cies that made use of them a superiority over their neighbors . small states , or states of less natural strength , under vigorous governments , and with the assistance of disciplined armies , have often triumph ##ed over large states , or states of greater natural strength , which have been des ##ti ##tute of these advantages . neither the pride nor the safety of the more important states or con ##fed ##era ##cies would permit them long to submit to this mort ##ifying and advent ##iti ##ous superiority . they would quickly resort to means similar to those by which it had been effect ##ed , to reins ##tate themselves in their lost pre - emi ##nen ##ce . thus , we should , in a little time , see established in every part of this country the same engines of des ##pot ##ism which have been the sc ##our ##ge of the old world . this , at least , would be the natural course of things ; and our reasoning ##s will be the more likely to be just , in proportion [SEP]\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:241] input_ids: 101 2027 2052 1010 2012 1996 2168 2051 1010 2022 29611 2000 12919 1996 3237 2849 1997 2231 1010 1999 2725 2029 2037 4552 2015 2052 9878 1037 6555 3257 2646 12078 1012 2009 2003 1997 1996 3267 1997 2162 2000 3623 1996 3237 2012 1996 10961 1997 1996 4884 3691 1012 1996 4654 5669 11638 2015 2029 2031 2042 3855 2052 2574 2507 1996 2163 2030 9530 25031 6906 9243 2008 2081 2224 1997 2068 1037 19113 2058 2037 10638 1012 2235 2163 1010 2030 2163 1997 2625 3019 3997 1010 2104 21813 6867 1010 1998 2007 1996 5375 1997 28675 8749 1010 2031 2411 10911 2098 2058 2312 2163 1010 2030 2163 1997 3618 3019 3997 1010 2029 2031 2042 4078 3775 24518 1997 2122 12637 1012 4445 1996 6620 4496 1996 3808 1997 1996 2062 2590 2163 2030 9530 25031 6906 9243 2052 9146 2068 2146 2000 12040 2000 2023 22294 11787 1998 13896 25090 3560 19113 1012 2027 2052 2855 7001 2000 2965 2714 2000 2216 2011 2029 2009 2018 2042 3466 2098 1010 2000 19222 12259 3209 1999 2037 2439 3653 1011 12495 10224 3401 1012 2947 1010 2057 2323 1010 1999 1037 2210 2051 1010 2156 2511 1999 2296 2112 1997 2023 2406 1996 2168 5209 1997 4078 11008 2964 2029 2031 2042 1996 8040 8162 3351 1997 1996 2214 2088 1012 2023 1010 2012 2560 1010 2052 2022 1996 3019 2607 1997 2477 1025 1998 2256 13384 2015 2097 2022 1996 2062 3497 2000 2022 2074 1010 1999 10817 102\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2023-10-26 09:45:58 text_classification_dataset:244] label: 0\n",
      "[NeMo W 2023-10-26 09:46:00 text_classification_dataset:250] Found 115 out of 115 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo I 2023-10-26 09:46:00 data_preprocessing:299] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2023-10-26 09:46:00 data_preprocessing:301] Min: 257 |                  Max: 257 |                  Mean: 257.0 |                  Median: 257.0\n",
      "[NeMo I 2023-10-26 09:46:00 data_preprocessing:307] 75 percentile: 257.00\n",
      "[NeMo I 2023-10-26 09:46:00 data_preprocessing:308] 99 percentile: 257.00\n",
      "[NeMo I 2023-10-26 09:46:00 text_classification_model:216] Dataloader config or file_path for the test is missing, so no data loader for test is created!\n",
      "[NeMo W 2023-10-26 09:46:00 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "[NeMo W 2023-10-26 09:46:00 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2023-10-26 09:46:01 text_classification_with_bert:118] ===========================================================================================\n",
      "[NeMo I 2023-10-26 09:46:01 text_classification_with_bert:119] Starting training...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2023-10-26 09:46:01 modelPT:748] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        eps: 1e-08\n",
      "        lr: 2e-05\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2023-10-26 09:46:01 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7f689be12910>\" \n",
      "    will be used during training (effective maximum steps = 240) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 240\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | loss                  | CrossEntropyLoss     | 0     \n",
      "1 | bert_model            | BertEncoder          | 109 M \n",
      "2 | classifier            | SequenceClassifier   | 592 K \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.297   Total estimated model params size (MB)\n",
      "Epoch 0:  80%|| 8/10 [00:04<00:01,  1.76it/s, loss=0.685, v_num=5-48, lr=5.6e-6\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|| 10/10 [00:04<00:00,  2.07it/s, loss=0.685, v_num=5-48, lr=5.6e-\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.22it/s]\u001b[A[NeMo I 2023-10-26 09:46:08 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 0: 100%|| 10/10 [00:04<00:00,  2.00it/s, loss=0.685, v_num=5-48, lr=6.4e-\n",
      "                                                                                \u001b[AEpoch 0, global step 7: val_loss reached 0.64341 (best 0.64341), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.64-epoch=0.ckpt\" as top 3\n",
      "Epoch 1:  80%|| 8/10 [00:04<00:01,  2.00it/s, loss=0.651, v_num=5-48, lr=1.2e-5\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|| 10/10 [00:04<00:00,  2.33it/s, loss=0.651, v_num=5-48, lr=1.2e-\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.11it/s]\u001b[A[NeMo I 2023-10-26 09:46:16 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 1: 100%|| 10/10 [00:04<00:00,  2.23it/s, loss=0.651, v_num=5-48, lr=1.28e\n",
      "                                                                                \u001b[AEpoch 1, global step 15: val_loss reached 0.57022 (best 0.57022), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.57-epoch=1.ckpt\" as top 3\n",
      "Epoch 2:  80%|| 8/10 [00:03<00:00,  2.01it/s, loss=0.612, v_num=5-48, lr=1.84e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|| 10/10 [00:04<00:00,  2.34it/s, loss=0.612, v_num=5-48, lr=1.84e\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.20it/s]\u001b[A[NeMo I 2023-10-26 09:46:24 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 2: 100%|| 10/10 [00:04<00:00,  2.25it/s, loss=0.612, v_num=5-48, lr=1.92e\n",
      "                                                                                \u001b[AEpoch 2, global step 23: val_loss reached 0.56904 (best 0.56904), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.57-epoch=2.ckpt\" as top 3\n",
      "Epoch 3:  80%|| 8/10 [00:03<00:00,  2.02it/s, loss=0.578, v_num=5-48, lr=1.94e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|| 10/10 [00:04<00:00,  2.35it/s, loss=0.578, v_num=5-48, lr=1.94e\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.10it/s]\u001b[A[NeMo I 2023-10-26 09:46:33 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 3: 100%|| 10/10 [00:04<00:00,  2.25it/s, loss=0.578, v_num=5-48, lr=1.94e\n",
      "                                                                                \u001b[AEpoch 3, global step 31: val_loss reached 0.55798 (best 0.55798), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.56-epoch=3.ckpt\" as top 3\n",
      "Epoch 4:  80%|| 8/10 [00:03<00:00,  2.02it/s, loss=0.566, v_num=5-48, lr=1.87e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|| 10/10 [00:04<00:00,  2.35it/s, loss=0.566, v_num=5-48, lr=1.87e\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.23it/s]\u001b[A[NeMo I 2023-10-26 09:46:41 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 4: 100%|| 10/10 [00:04<00:00,  2.26it/s, loss=0.566, v_num=5-48, lr=1.86e\n",
      "                                                                                \u001b[AEpoch 4, global step 39: val_loss reached 0.55208 (best 0.55208), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.55-epoch=4.ckpt\" as top 3\n",
      "Epoch 5:  80%|| 8/10 [00:03<00:00,  2.02it/s, loss=0.566, v_num=5-48, lr=1.8e-5\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|| 10/10 [00:04<00:00,  2.35it/s, loss=0.566, v_num=5-48, lr=1.8e-\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.21it/s]\u001b[A[NeMo I 2023-10-26 09:46:50 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             77.39     100.00      87.25         89\n",
      "    label_id: 1                                              0.00       0.00       0.00         26\n",
      "    -------------------\n",
      "    micro avg                                               77.39      77.39      77.39        115\n",
      "    macro avg                                               38.70      50.00      43.63        115\n",
      "    weighted avg                                            59.89      77.39      67.53        115\n",
      "    \n",
      "Epoch 5: 100%|| 10/10 [00:04<00:00,  2.26it/s, loss=0.566, v_num=5-48, lr=1.79e\n",
      "                                                                                \u001b[AEpoch 5, global step 47: val_loss reached 0.53721 (best 0.53721), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.54-epoch=5.ckpt\" as top 3\n",
      "Epoch 6:  80%|| 8/10 [00:03<00:00,  2.00it/s, loss=0.545, v_num=5-48, lr=1.72e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|| 10/10 [00:04<00:00,  2.33it/s, loss=0.545, v_num=5-48, lr=1.72e\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.18it/s]\u001b[A[NeMo I 2023-10-26 09:46:59 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             78.07     100.00      87.68         89\n",
      "    label_id: 1                                            100.00       3.85       7.41         26\n",
      "    -------------------\n",
      "    micro avg                                               78.26      78.26      78.26        115\n",
      "    macro avg                                               89.04      51.92      47.55        115\n",
      "    weighted avg                                            83.03      78.26      69.54        115\n",
      "    \n",
      "Epoch 6: 100%|| 10/10 [00:04<00:00,  2.24it/s, loss=0.545, v_num=5-48, lr=1.71e\n",
      "                                                                                \u001b[AEpoch 6, global step 55: val_loss reached 0.51701 (best 0.51701), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.52-epoch=6.ckpt\" as top 3\n",
      "Epoch 7:  80%|| 8/10 [00:03<00:00,  2.00it/s, loss=0.522, v_num=5-48, lr=1.65e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|| 10/10 [00:04<00:00,  2.33it/s, loss=0.522, v_num=5-48, lr=1.65e\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.17it/s]\u001b[A[NeMo I 2023-10-26 09:47:07 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             80.91     100.00      89.45         89\n",
      "    label_id: 1                                            100.00      19.23      32.26         26\n",
      "    -------------------\n",
      "    micro avg                                               81.74      81.74      81.74        115\n",
      "    macro avg                                               90.45      59.62      60.85        115\n",
      "    weighted avg                                            85.23      81.74      76.52        115\n",
      "    \n",
      "Epoch 7: 100%|| 10/10 [00:04<00:00,  2.24it/s, loss=0.522, v_num=5-48, lr=1.64e\n",
      "                                                                                \u001b[AEpoch 7, global step 63: val_loss reached 0.48978 (best 0.48978), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.49-epoch=7.ckpt\" as top 3\n",
      "Epoch 8:  80%|| 8/10 [00:03<00:00,  2.00it/s, loss=0.482, v_num=5-48, lr=1.57e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|| 10/10 [00:04<00:00,  2.33it/s, loss=0.482, v_num=5-48, lr=1.57e\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.20it/s]\u001b[A[NeMo I 2023-10-26 09:47:16 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             82.69      96.63      89.12         89\n",
      "    label_id: 1                                             72.73      30.77      43.24         26\n",
      "    -------------------\n",
      "    micro avg                                               81.74      81.74      81.74        115\n",
      "    macro avg                                               77.71      63.70      66.18        115\n",
      "    weighted avg                                            80.44      81.74      78.75        115\n",
      "    \n",
      "Epoch 8: 100%|| 10/10 [00:04<00:00,  2.24it/s, loss=0.482, v_num=5-48, lr=1.56e\n",
      "                                                                                \u001b[AEpoch 8, global step 71: val_loss reached 0.45635 (best 0.45635), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.46-epoch=8.ckpt\" as top 3\n",
      "Epoch 9:  80%|| 8/10 [00:04<00:01,  2.00it/s, loss=0.424, v_num=5-48, lr=1.5e-5\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|| 10/10 [00:04<00:00,  2.33it/s, loss=0.424, v_num=5-48, lr=1.5e-\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.21it/s]\u001b[A[NeMo I 2023-10-26 09:47:25 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             84.16      95.51      89.47         89\n",
      "    label_id: 1                                             71.43      38.46      50.00         26\n",
      "    -------------------\n",
      "    micro avg                                               82.61      82.61      82.61        115\n",
      "    macro avg                                               77.79      66.98      69.74        115\n",
      "    weighted avg                                            81.28      82.61      80.55        115\n",
      "    \n",
      "Epoch 9: 100%|| 10/10 [00:04<00:00,  2.24it/s, loss=0.424, v_num=5-48, lr=1.49e\n",
      "                                                                                \u001b[AEpoch 9, global step 79: val_loss reached 0.42654 (best 0.42654), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.43-epoch=9.ckpt\" as top 3\n",
      "Epoch 10:  80%|| 8/10 [00:03<00:00,  2.00it/s, loss=0.355, v_num=5-48, lr=1.43e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|| 10/10 [00:04<00:00,  2.33it/s, loss=0.355, v_num=5-48, lr=1.43\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.17it/s]\u001b[A[NeMo I 2023-10-26 09:47:34 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             88.17      92.13      90.11         89\n",
      "    label_id: 1                                             68.18      57.69      62.50         26\n",
      "    -------------------\n",
      "    micro avg                                               84.35      84.35      84.35        115\n",
      "    macro avg                                               78.18      74.91      76.30        115\n",
      "    weighted avg                                            83.65      84.35      83.87        115\n",
      "    \n",
      "Epoch 10: 100%|| 10/10 [00:04<00:00,  2.24it/s, loss=0.355, v_num=5-48, lr=1.42\n",
      "                                                                                \u001b[AEpoch 10, global step 87: val_loss reached 0.39340 (best 0.39340), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.39-epoch=10.ckpt\" as top 3\n",
      "Epoch 11:  80%|| 8/10 [00:03<00:00,  2.02it/s, loss=0.274, v_num=5-48, lr=1.35e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|| 10/10 [00:04<00:00,  2.35it/s, loss=0.274, v_num=5-48, lr=1.35\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.19it/s]\u001b[A[NeMo I 2023-10-26 09:47:42 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             86.00      96.63      91.01         89\n",
      "    label_id: 1                                             80.00      46.15      58.54         26\n",
      "    -------------------\n",
      "    micro avg                                               85.22      85.22      85.22        115\n",
      "    macro avg                                               83.00      71.39      74.77        115\n",
      "    weighted avg                                            84.64      85.22      83.66        115\n",
      "    \n",
      "Epoch 11: 100%|| 10/10 [00:04<00:00,  2.25it/s, loss=0.274, v_num=5-48, lr=1.34\n",
      "                                                                                \u001b[AEpoch 11, global step 95: val_loss reached 0.42742 (best 0.39340), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.43-epoch=11.ckpt\" as top 3\n",
      "Epoch 12:  80%|| 8/10 [00:03<00:00,  2.02it/s, loss=0.201, v_num=5-48, lr=1.28e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|| 10/10 [00:04<00:00,  2.35it/s, loss=0.201, v_num=5-48, lr=1.28\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.21it/s]\u001b[A[NeMo I 2023-10-26 09:47:51 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             89.13      92.13      90.61         89\n",
      "    label_id: 1                                             69.57      61.54      65.31         26\n",
      "    -------------------\n",
      "    micro avg                                               85.22      85.22      85.22        115\n",
      "    macro avg                                               79.35      76.84      77.96        115\n",
      "    weighted avg                                            84.71      85.22      84.89        115\n",
      "    \n",
      "Epoch 12: 100%|| 10/10 [00:04<00:00,  2.26it/s, loss=0.201, v_num=5-48, lr=1.27\n",
      "                                                                                \u001b[AEpoch 12, global step 103: val_loss reached 0.38197 (best 0.38197), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.38-epoch=12.ckpt\" as top 3\n",
      "Epoch 13:  80%|| 8/10 [00:03<00:00,  2.01it/s, loss=0.136, v_num=5-48, lr=1.2e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|| 10/10 [00:04<00:00,  2.34it/s, loss=0.136, v_num=5-48, lr=1.2e\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.09it/s]\u001b[A[NeMo I 2023-10-26 09:48:00 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             89.13      92.13      90.61         89\n",
      "    label_id: 1                                             69.57      61.54      65.31         26\n",
      "    -------------------\n",
      "    micro avg                                               85.22      85.22      85.22        115\n",
      "    macro avg                                               79.35      76.84      77.96        115\n",
      "    weighted avg                                            84.71      85.22      84.89        115\n",
      "    \n",
      "Epoch 13: 100%|| 10/10 [00:04<00:00,  2.25it/s, loss=0.136, v_num=5-48, lr=1.19\n",
      "                                                                                \u001b[AEpoch 13, global step 111: val_loss reached 0.38865 (best 0.38197), saving model to \"/dli/task/nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification--val_loss=0.39-epoch=13.ckpt\" as top 3\n",
      "Epoch 14:  80%|| 8/10 [00:03<00:00,  2.01it/s, loss=0.0938, v_num=5-48, lr=1.13\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|| 10/10 [00:04<00:00,  2.34it/s, loss=0.0938, v_num=5-48, lr=1.1\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.09it/s]\u001b[A[NeMo I 2023-10-26 09:48:09 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             88.54      95.51      91.89         89\n",
      "    label_id: 1                                             78.95      57.69      66.67         26\n",
      "    -------------------\n",
      "    micro avg                                               86.96      86.96      86.96        115\n",
      "    macro avg                                               83.74      76.60      79.28        115\n",
      "    weighted avg                                            86.37      86.96      86.19        115\n",
      "    \n",
      "Epoch 14: 100%|| 10/10 [00:04<00:00,  2.25it/s, loss=0.0938, v_num=5-48, lr=1.1\n",
      "                                                                                \u001b[AEpoch 14, step 119: val_loss was not in top 3\n",
      "Epoch 15:  80%|| 8/10 [00:03<00:00,  2.00it/s, loss=0.0637, v_num=5-48, lr=1.06\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|| 10/10 [00:04<00:00,  2.33it/s, loss=0.0637, v_num=5-48, lr=1.0\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.10it/s]\u001b[A[NeMo I 2023-10-26 09:48:15 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             89.36      94.38      91.80         89\n",
      "    label_id: 1                                             76.19      61.54      68.09         26\n",
      "    -------------------\n",
      "    micro avg                                               86.96      86.96      86.96        115\n",
      "    macro avg                                               82.78      77.96      79.94        115\n",
      "    weighted avg                                            86.38      86.96      86.44        115\n",
      "    \n",
      "Epoch 15: 100%|| 10/10 [00:04<00:00,  2.24it/s, loss=0.0637, v_num=5-48, lr=1.0\n",
      "                                                                                \u001b[AEpoch 15, step 127: val_loss was not in top 3\n",
      "Epoch 16:  80%|| 8/10 [00:03<00:00,  2.02it/s, loss=0.0443, v_num=5-48, lr=9.81\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|| 10/10 [00:04<00:00,  2.35it/s, loss=0.0443, v_num=5-48, lr=9.8\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.22it/s]\u001b[A[NeMo I 2023-10-26 09:48:22 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             89.47      95.51      92.39         89\n",
      "    label_id: 1                                             80.00      61.54      69.57         26\n",
      "    -------------------\n",
      "    micro avg                                               87.83      87.83      87.83        115\n",
      "    macro avg                                               84.74      78.52      80.98        115\n",
      "    weighted avg                                            87.33      87.83      87.23        115\n",
      "    \n",
      "Epoch 16: 100%|| 10/10 [00:04<00:00,  2.26it/s, loss=0.0443, v_num=5-48, lr=9.7\n",
      "                                                                                \u001b[AEpoch 16, step 135: val_loss was not in top 3\n",
      "Epoch 17:  80%|| 8/10 [00:03<00:00,  2.02it/s, loss=0.0359, v_num=5-48, lr=9.07\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|| 10/10 [00:04<00:00,  2.35it/s, loss=0.0359, v_num=5-48, lr=9.0\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.17it/s]\u001b[A[NeMo I 2023-10-26 09:48:28 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             92.05      91.01      91.53         89\n",
      "    label_id: 1                                             70.37      73.08      71.70         26\n",
      "    -------------------\n",
      "    micro avg                                               86.96      86.96      86.96        115\n",
      "    macro avg                                               81.21      82.04      81.61        115\n",
      "    weighted avg                                            87.15      86.96      87.04        115\n",
      "    \n",
      "Epoch 17: 100%|| 10/10 [00:04<00:00,  2.25it/s, loss=0.0359, v_num=5-48, lr=8.9\n",
      "                                                                                \u001b[AEpoch 17, step 143: val_loss was not in top 3\n",
      "Epoch 18:  80%|| 8/10 [00:03<00:00,  2.01it/s, loss=0.03, v_num=5-48, lr=8.33e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|| 10/10 [00:04<00:00,  2.34it/s, loss=0.03, v_num=5-48, lr=8.33e\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.20it/s]\u001b[A[NeMo I 2023-10-26 09:48:35 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             88.54      95.51      91.89         89\n",
      "    label_id: 1                                             78.95      57.69      66.67         26\n",
      "    -------------------\n",
      "    micro avg                                               86.96      86.96      86.96        115\n",
      "    macro avg                                               83.74      76.60      79.28        115\n",
      "    weighted avg                                            86.37      86.96      86.19        115\n",
      "    \n",
      "Epoch 18: 100%|| 10/10 [00:04<00:00,  2.25it/s, loss=0.03, v_num=5-48, lr=8.24e\n",
      "                                                                                \u001b[AEpoch 18, step 151: val_loss was not in top 3\n",
      "Epoch 19:  80%|| 8/10 [00:04<00:01,  2.00it/s, loss=0.0256, v_num=5-48, lr=7.59\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|| 10/10 [00:04<00:00,  2.33it/s, loss=0.0256, v_num=5-48, lr=7.5\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.21it/s]\u001b[A[NeMo I 2023-10-26 09:48:41 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             87.76      96.63      91.98         89\n",
      "    label_id: 1                                             82.35      53.85      65.12         26\n",
      "    -------------------\n",
      "    micro avg                                               86.96      86.96      86.96        115\n",
      "    macro avg                                               85.05      75.24      78.55        115\n",
      "    weighted avg                                            86.53      86.96      85.91        115\n",
      "    \n",
      "Epoch 19: 100%|| 10/10 [00:04<00:00,  2.24it/s, loss=0.0256, v_num=5-48, lr=7.5\n",
      "                                                                                \u001b[AEpoch 19, step 159: val_loss was not in top 3\n",
      "Epoch 20:  80%|| 8/10 [00:03<00:00,  2.01it/s, loss=0.0222, v_num=5-48, lr=6.85\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|| 10/10 [00:04<00:00,  2.35it/s, loss=0.0222, v_num=5-48, lr=6.8\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.20it/s]\u001b[A[NeMo I 2023-10-26 09:48:48 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             90.32      94.38      92.31         89\n",
      "    label_id: 1                                             77.27      65.38      70.83         26\n",
      "    -------------------\n",
      "    micro avg                                               87.83      87.83      87.83        115\n",
      "    macro avg                                               83.80      79.88      81.57        115\n",
      "    weighted avg                                            87.37      87.83      87.45        115\n",
      "    \n",
      "Epoch 20: 100%|| 10/10 [00:04<00:00,  2.25it/s, loss=0.0222, v_num=5-48, lr=6.7\n",
      "                                                                                \u001b[AEpoch 20, step 167: val_loss was not in top 3\n",
      "Epoch 21:  80%|| 8/10 [00:03<00:00,  2.01it/s, loss=0.02, v_num=5-48, lr=6.11e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|| 10/10 [00:04<00:00,  2.33it/s, loss=0.02, v_num=5-48, lr=6.11e\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.10it/s]\u001b[A[NeMo I 2023-10-26 09:48:54 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             87.76      96.63      91.98         89\n",
      "    label_id: 1                                             82.35      53.85      65.12         26\n",
      "    -------------------\n",
      "    micro avg                                               86.96      86.96      86.96        115\n",
      "    macro avg                                               85.05      75.24      78.55        115\n",
      "    weighted avg                                            86.53      86.96      85.91        115\n",
      "    \n",
      "Epoch 21: 100%|| 10/10 [00:04<00:00,  2.24it/s, loss=0.02, v_num=5-48, lr=6.02e\n",
      "                                                                                \u001b[AEpoch 21, step 175: val_loss was not in top 3\n",
      "Epoch 22:  80%|| 8/10 [00:03<00:00,  2.00it/s, loss=0.0196, v_num=5-48, lr=5.37\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|| 10/10 [00:04<00:00,  2.33it/s, loss=0.0196, v_num=5-48, lr=5.3\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.23it/s]\u001b[A[NeMo I 2023-10-26 09:49:01 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             87.76      96.63      91.98         89\n",
      "    label_id: 1                                             82.35      53.85      65.12         26\n",
      "    -------------------\n",
      "    micro avg                                               86.96      86.96      86.96        115\n",
      "    macro avg                                               85.05      75.24      78.55        115\n",
      "    weighted avg                                            86.53      86.96      85.91        115\n",
      "    \n",
      "Epoch 22: 100%|| 10/10 [00:04<00:00,  2.24it/s, loss=0.0196, v_num=5-48, lr=5.2\n",
      "                                                                                \u001b[AEpoch 22, step 183: val_loss was not in top 3\n",
      "Epoch 23:  80%|| 8/10 [00:03<00:00,  2.01it/s, loss=0.0194, v_num=5-48, lr=4.63\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|| 10/10 [00:04<00:00,  2.34it/s, loss=0.0194, v_num=5-48, lr=4.6\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.21it/s]\u001b[A[NeMo I 2023-10-26 09:49:07 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             90.11      92.13      91.11         89\n",
      "    label_id: 1                                             70.83      65.38      68.00         26\n",
      "    -------------------\n",
      "    micro avg                                               86.09      86.09      86.09        115\n",
      "    macro avg                                               80.47      78.76      79.56        115\n",
      "    weighted avg                                            85.75      86.09      85.89        115\n",
      "    \n",
      "Epoch 23: 100%|| 10/10 [00:04<00:00,  2.25it/s, loss=0.0194, v_num=5-48, lr=4.5\n",
      "                                                                                \u001b[AEpoch 23, step 191: val_loss was not in top 3\n",
      "Epoch 24:  80%|| 8/10 [00:03<00:00,  2.02it/s, loss=0.0188, v_num=5-48, lr=3.89\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|| 10/10 [00:04<00:00,  2.35it/s, loss=0.0188, v_num=5-48, lr=3.8\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.21it/s]\u001b[A[NeMo I 2023-10-26 09:49:14 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             87.63      95.51      91.40         89\n",
      "    label_id: 1                                             77.78      53.85      63.64         26\n",
      "    -------------------\n",
      "    micro avg                                               86.09      86.09      86.09        115\n",
      "    macro avg                                               82.70      74.68      77.52        115\n",
      "    weighted avg                                            85.40      86.09      85.12        115\n",
      "    \n",
      "Epoch 24: 100%|| 10/10 [00:04<00:00,  2.25it/s, loss=0.0188, v_num=5-48, lr=3.8\n",
      "                                                                                \u001b[AEpoch 24, step 199: val_loss was not in top 3\n",
      "Epoch 25:  80%|| 8/10 [00:04<00:01,  2.00it/s, loss=0.0179, v_num=5-48, lr=3.15\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|| 10/10 [00:04<00:00,  2.33it/s, loss=0.0179, v_num=5-48, lr=3.1\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.18it/s]\u001b[A[NeMo I 2023-10-26 09:49:20 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             87.63      95.51      91.40         89\n",
      "    label_id: 1                                             77.78      53.85      63.64         26\n",
      "    -------------------\n",
      "    micro avg                                               86.09      86.09      86.09        115\n",
      "    macro avg                                               82.70      74.68      77.52        115\n",
      "    weighted avg                                            85.40      86.09      85.12        115\n",
      "    \n",
      "Epoch 25: 100%|| 10/10 [00:04<00:00,  2.23it/s, loss=0.0179, v_num=5-48, lr=3.0\n",
      "                                                                                \u001b[AEpoch 25, step 207: val_loss was not in top 3\n",
      "Epoch 26:  80%|| 8/10 [00:03<00:00,  2.00it/s, loss=0.0165, v_num=5-48, lr=2.41\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|| 10/10 [00:04<00:00,  2.34it/s, loss=0.0165, v_num=5-48, lr=2.4\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.22it/s]\u001b[A[NeMo I 2023-10-26 09:49:27 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             87.63      95.51      91.40         89\n",
      "    label_id: 1                                             77.78      53.85      63.64         26\n",
      "    -------------------\n",
      "    micro avg                                               86.09      86.09      86.09        115\n",
      "    macro avg                                               82.70      74.68      77.52        115\n",
      "    weighted avg                                            85.40      86.09      85.12        115\n",
      "    \n",
      "Epoch 26: 100%|| 10/10 [00:04<00:00,  2.24it/s, loss=0.0165, v_num=5-48, lr=2.3\n",
      "                                                                                \u001b[AEpoch 26, step 215: val_loss was not in top 3\n",
      "Epoch 27:  80%|| 8/10 [00:03<00:00,  2.01it/s, loss=0.016, v_num=5-48, lr=1.67e\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|| 10/10 [00:04<00:00,  2.34it/s, loss=0.016, v_num=5-48, lr=1.67\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.22it/s]\u001b[A[NeMo I 2023-10-26 09:49:33 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             87.63      95.51      91.40         89\n",
      "    label_id: 1                                             77.78      53.85      63.64         26\n",
      "    -------------------\n",
      "    micro avg                                               86.09      86.09      86.09        115\n",
      "    macro avg                                               82.70      74.68      77.52        115\n",
      "    weighted avg                                            85.40      86.09      85.12        115\n",
      "    \n",
      "Epoch 27: 100%|| 10/10 [00:04<00:00,  2.25it/s, loss=0.016, v_num=5-48, lr=1.57\n",
      "                                                                                \u001b[AEpoch 27, step 223: val_loss was not in top 3\n",
      "Epoch 28:  80%|| 8/10 [00:03<00:00,  2.01it/s, loss=0.0153, v_num=5-48, lr=9.26\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28: 100%|| 10/10 [00:04<00:00,  2.34it/s, loss=0.0153, v_num=5-48, lr=9.2\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.16it/s]\u001b[A[NeMo I 2023-10-26 09:49:40 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             87.63      95.51      91.40         89\n",
      "    label_id: 1                                             77.78      53.85      63.64         26\n",
      "    -------------------\n",
      "    micro avg                                               86.09      86.09      86.09        115\n",
      "    macro avg                                               82.70      74.68      77.52        115\n",
      "    weighted avg                                            85.40      86.09      85.12        115\n",
      "    \n",
      "Epoch 28: 100%|| 10/10 [00:04<00:00,  2.24it/s, loss=0.0153, v_num=5-48, lr=8.3\n",
      "                                                                                \u001b[AEpoch 28, step 231: val_loss was not in top 3\n",
      "Epoch 29:  80%|| 8/10 [00:03<00:00,  2.00it/s, loss=0.0154, v_num=5-48, lr=1.85\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|| 10/10 [00:04<00:00,  2.33it/s, loss=0.0154, v_num=5-48, lr=1.8\u001b[A\n",
      "Validating: 100%|| 2/2 [00:00<00:00,  5.19it/s]\u001b[A[NeMo I 2023-10-26 09:49:46 text_classification_model:165] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             87.63      95.51      91.40         89\n",
      "    label_id: 1                                             77.78      53.85      63.64         26\n",
      "    -------------------\n",
      "    micro avg                                               86.09      86.09      86.09        115\n",
      "    macro avg                                               82.70      74.68      77.52        115\n",
      "    weighted avg                                            85.40      86.09      85.12        115\n",
      "    \n",
      "Epoch 29: 100%|| 10/10 [00:04<00:00,  2.24it/s, loss=0.0154, v_num=5-48, lr=9.2\n",
      "                                                                                \u001b[AEpoch 29, step 239: val_loss was not in top 3\n",
      "Saving latest checkpoint...\n",
      "Epoch 29: 100%|| 10/10 [00:06<00:00,  1.54it/s, loss=0.0154, v_num=5-48, lr=9.2\n",
      "[NeMo W 2023-10-26 09:49:48 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n",
      "[NeMo I 2023-10-26 09:50:11 text_classification_with_bert:121] Training finished!\n",
      "[NeMo I 2023-10-26 09:50:11 text_classification_with_bert:122] ===========================================================================================\n",
      "[NeMo I 2023-10-26 09:50:35 text_classification_with_bert:127] Model is saved into `.nemo` file: text_classification_model.nemo\n",
      "CPU times: user 2.53 s, sys: 1.21 s, total: 3.74 s\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the training script, overriding the config values in the command line\n",
    "TC_DIR = \"/dli/task/nemo/examples/nlp/text_classification\"\n",
    "\n",
    "\n",
    "!python $TC_DIR/text_classification_with_bert.py \\\n",
    "        model.dataset.num_classes=$NUM_CLASSES \\\n",
    "        model.dataset.max_seq_length=$MAX_SEQ_LENGTH \\\n",
    "        model.train_ds.file_path=$PATH_TO_TRAIN_FILE \\\n",
    "        model.validation_ds.file_path=$PATH_TO_VAL_FILE \\\n",
    "        model.infer_samples=[] \\\n",
    "        trainer.max_epochs=$MAX_EPOCHS \\\n",
    "        model.language_model.pretrained_model_name=$PRETRAINED_MODEL_NAME \\\n",
    "        trainer.gpus=1 \\\n",
    "        trainer.amp_level=$AMP_LEVEL \\\n",
    "        trainer.precision=$PRECISION \\\n",
    "        model.train_ds.batch_size=$BATCH_SIZE \\\n",
    "        model.validation_ds.batch_size=$BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "cmd_log = os.path.join(os.path.dirname(os.path.dirname(get_latest_model())),'cmd-args.log')\n",
    "lightning_logs = os.path.join(os.path.dirname(os.path.dirname(get_latest_model())),'lightning_logs.txt')\n",
    "\n",
    "with open(cmd_log, \"r\") as f:\n",
    "    cmd = f.read()\n",
    "    cmd_list = cmd.split()\n",
    "with open(\"my_assessment/step4.json\", \"w\") as outfile: \n",
    "    json.dump(cmd_list, outfile) \n",
    "    \n",
    "with open(lightning_logs, \"r\") as f:\n",
    "    log = f.readlines()\n",
    "with open(\"my_assessment/step4_lightning.json\", \"w\") as outfile:\n",
    "    json.dump(log, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 5: Infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference (graded)\n",
    "Run the inference blocks to see and save the results. (Note: there is nothing to fix here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "[NeMo W 2023-10-26 09:50:42 modelPT:137] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    file_path: /dli/task/data/federalist_papers_HM/train_nemo_format.tsv\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2023-10-26 09:50:42 modelPT:144] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    file_path: /dli/task/data/federalist_papers_HM/dev_nemo_format.tsv\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2023-10-26 09:50:42 modelPT:151] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n",
      "[NeMo W 2023-10-26 09:50:42 modelPT:1198] World size can only be set by PyTorch Lightning Trainer.\n",
      "[NeMo W 2023-10-26 09:50:42 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "[NeMo W 2023-10-26 09:50:42 modelPT:197] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact forit has already been registered.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-10-26 09:50:45 modelPT:434] Model TextClassificationModel was successfully restored from nemo_experiments/TextClassification/2023-10-26_09-45-48/checkpoints/TextClassification.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-10-26 09:50:45 text_classification_dataset:250] Found 7 out of 7 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2023-10-26 09:50:46 text_classification_dataset:250] Found 4 out of 4 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2023-10-26 09:50:47 text_classification_dataset:250] Found 7 out of 8 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2023-10-26 09:50:47 text_classification_dataset:250] Found 7 out of 7 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2023-10-26 09:50:47 text_classification_dataset:250] Found 9 out of 9 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2023-10-26 09:50:48 text_classification_dataset:250] Found 8 out of 8 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2023-10-26 09:50:48 text_classification_dataset:250] Found 8 out of 8 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2023-10-26 09:50:49 text_classification_dataset:250] Found 6 out of 6 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2023-10-26 09:50:49 text_classification_dataset:250] Found 9 out of 9 sentences with more than 256 subtokens. Truncated long sentences from the end.\n",
      "[NeMo W 2023-10-26 09:50:50 text_classification_dataset:250] Found 22 out of 22 sentences with more than 256 subtokens. Truncated long sentences from the end.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 0, 0, 1, 0, 0], [0, 0, 1, 1], [0, 0, 0, 1, 1, 1, 1, 0], [1, 1, 0, 1, 1, 0, 1], [0, 1, 1, 0, 1, 1, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1], [1, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0, 1, 0], [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Run inference for assessment -  - DO NOT CHANGE\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "# Instantiate the model by restoring from the latest .nemo checkpoint\n",
    "model = nemo_nlp.models.TextClassificationModel.restore_from(get_latest_model())\n",
    "\n",
    "# Find the latest model path\n",
    "DATA_DIR = '/dli/task/data/federalist_papers_HM'\n",
    "\n",
    "test_files = [\n",
    "    'test49.tsv',\n",
    "    'test50.tsv',\n",
    "    'test51.tsv',\n",
    "    'test52.tsv',\n",
    "    'test53.tsv',\n",
    "    'test54.tsv', \n",
    "    'test55.tsv',\n",
    "    'test56.tsv',\n",
    "    'test57.tsv',\n",
    "    'test62.tsv',\n",
    "]\n",
    "results = []\n",
    "for test_file in test_files:\n",
    "    # get as list and remove header row\n",
    "    filepath = os.path.join(DATA_DIR, test_file)\n",
    "    with open(filepath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    del lines[0]\n",
    "    \n",
    "    results.append(model.classifytext(lines, batch_size = 1, max_seq_length = 256))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAMILTON\n",
      "MADISON\n",
      "MADISON\n",
      "MADISON\n",
      "HAMILTON\n",
      "MADISON\n",
      "HAMILTON\n",
      "HAMILTON\n",
      "HAMILTON\n",
      "HAMILTON\n"
     ]
    }
   ],
   "source": [
    "# Run to save for assessment- DO NOT CHANGE\n",
    "author = []\n",
    "for result in results:\n",
    "    avg_result = sum(result) / len(result)\n",
    "    if avg_result < 0.5:\n",
    "        author.append(\"HAMILTON\")\n",
    "        print(\"HAMILTON\")\n",
    "    else:\n",
    "        author.append(\"MADISON\")\n",
    "        print(\"MADISON\")\n",
    "        \n",
    "with open(\"my_assessment/step5.json\", \"w\") as outfile: \n",
    "    json.dump(author, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 6: Submit You Assessment\n",
    "How were your results?  According to an earlier [machine learning analysis using support vector machines](http://pages.cs.wisc.edu/~gfung/federalist.pdf), Madison was the most likely true author of all the disputed papers (assuming no collaboration).  It is possible to get the \"all MADISON\" answer using the tools you have.  If you are so inclined, you can keep trying, though **a particular result is *NOT* required to pass the assessment**.\n",
    "\n",
    "If you are satisfied that you have completed the code correctly, and that your training and inference are working correctly, you can submit your project as follows to the autograder:\n",
    "\n",
    "1. Go back to the GPU launch page and click the checkmark to run the assessment:\n",
    "\n",
    "<img src=\"images/assessment_checkmark.png\">\n",
    "\n",
    "2. That's it!  If you passed, you'll receive a pop-up window saying so, and the points will be credited to your progress.  If not, you'll receive feedback in the pop-up window. \n",
    "\n",
    "<img src=\"images/assessment_pass_popup.png\">\n",
    "\n",
    "You can always check your assessment progress in the course progress tab.  Note that partial values for the coding assessment won't be visible here - it shows up as either 0 or 70 points.  Be sure to complete the questions on Transformer and Deployment on the same course page to qualify for your final certificate!\n",
    "\n",
    "<img src=\"images/progress.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
